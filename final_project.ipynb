{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>__Final Project: Global Warming__</center> #\n",
    "## <center>__Data Mining (CSCI4502)__</center> ##\n",
    "### <center>__Ravie Starzl__</center> ###\n",
    "### <center>__Tyler Scrips & Angel Romero__</center> ###\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Motivation</center> #####\n",
    "The motivation for us to mine data revolving around the problem that is Global Warming is straight-forward, Global Warming is a big issue just because it is affecting us in a wide variety of ways. This problem needs to be a priority when it comes to global crises. Global warming is a serious topic and must be addressed in a serious manner. Extracting and manipulating data to represent the problems we will experience going forward is our goal in this project, as well as display statistical information revolving around contributing factors of global warming.\n",
    "\n",
    "We wanted to take on the topic of Global Warming because of the overall attention that it gets but without having to look too hard on a topic that barely grazes the minds of others. We also had questions of our own on how the earth is affected by certain factors that are/aren't addressed, this composes of volcanic activity, automobile usage, and human-engineered environments that pollute the air with co2. We wanted to also know who is the bigger producer and why they hold that place. \n",
    "\n",
    "These many questions drove our attention to certain factors that can give us answers on what makes accumulates co2_emmissions and why. This is why we chose our topic.\n",
    "\n",
    "##### <center>Why is Global Warming causing us issues?</center> #####\n",
    "The increase of temperatures has caused many issues, ice-caps melting has led to sea level rise, disruption of ecosystems that would disrupt species mortality rate that reside within, health issues, food and water scarcity, etc. Due to the amount of problems that global warming is already causing, we can represent this data visually or concatenate this to a dataset that would represent it in a much more complex manner.\n",
    "\n",
    "As many people already know, if co2 emmissions arise and Global Warming gets worse, this can drastically change everything around us and it can only go down from here if we don't do anything now.\n",
    "\n",
    "Observing changes in temperature will help us determine who will be most affected by climate change going forward. By combining that data with emissions data we can see how emissions may have localized effects on regional patterns and hopefully provide further predictive ability for events like heat waves or even cold snaps that can have major health impacts on local communities.\n",
    "\n",
    "##### <center>Why should we care?</center> #####\n",
    "Well, we all live on the same planet. All jokes aside, collecting this type of data can be used to determine who is the most responsible when it comes to CO2 emissions output. The amount of specified data can help us speculate what we can predict in the near future, such as future trends, or we can ‘splice’ up CO2 contributions into CO2 factors. Collecting this type of data can help Researchers conduct mitigation methods that would mitigate the amount of CO2 for leading causes of CO2, even if us humans are the main cause. \n",
    "\n",
    "The process will conduct an analysis of the data collected from the datasets from Kaggle.com. We will also conduct research on other factors that contribute to the upscale of global warming; gas-powered vehicles, volcanic eruptions, catastrophic events, environmental changes, CO2 emissions output from each country (if we delve deeper, maybe states?), etc. From this data, we can create a stack of factors that go into a much more comprehensive detailed data set that represents these factors accordingly on a time frame that is represented incrementally.\n",
    "\n",
    "#### <center>Literature Survey</center> ####\n",
    "The daily temperature dataset from kaggle has a significant body of work done by other contributors while the emissions dataset is a little more sparse regarding kaggle. This would include data sets in .csv/JSON/SQL/Jupyter Notebook format, discussions revolving around said dataset/topic, code that others have shared among the public that shows their own manipulation via the dataset given, and competitions that would pay actual money to the person who can manipulate and represent the data the best among other competitors.\n",
    "\n",
    "It’s pretty straightforward that we are going to use the datasets provided from the website that others have recorded but the code is also very interesting and something we might reference. The code shows how others manipulate the data set to get desired output or represent the data in a more intricate manner. This can range from having the x-axis as the time frame in increments and the y-axis as something they choose. It can be represented as different plots and graphs. There were also really cool ideas that involved different use of graphs(such as a radar chart), different correlations that the data represented in coherence with seasons, spectacles from the data they extracted from the dataset (such as future trend lines), etc. Ranking was prevalent in the code of others, where the most contributive country/region would output the most values, especially relating to CO2 emissions.\n",
    "\n",
    "Discussions also serve an important purpose, they provide insight on misunderstandings of the datasets. Sometimes people extend on the information presented and try to help others understand the data in a much more intricate manner.\n",
    "\n",
    "From the Discussions, we were able to see how other people were able to visualize, clean, and manipulate the datasets from the csv_files provided. Everyone had their own take on it and were able to come up with consensuses by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Initializing all the necessary libraries...</center> ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as pex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import plotly.offline as pyo\n",
    "from os.path import exists\n",
    "\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>__Data Cleaning__</center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> All datasets that we are referencing are all cited within the file named, 'datasets/'. Where we display all of the datasets used, where they came from, the authors, and a link to reference the page where we collected the dataset from kaggle from.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Trend Cleaning:\n",
    "We collected the data that we needs in order to display future trends. One of the datasets called, 'CO2_emission.csv' in which we utliized the melt() method from the pandas library. The reason for this is quite straight forward, there seems to be columns that have a year heading from 1990 - 2016. So, we would 'melt' these values together in order to fit into 1 single column, that being the co2_metrics per state/country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Country Name', 'country_code', 'Region', 'Indicator Name', '1990',\n",
      "       '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999',\n",
      "       '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008',\n",
      "       '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017',\n",
      "       '2018', '2019', '2019.1'],\n",
      "      dtype='object')\n",
      "(215, 35)\n",
      "             1990        1991        1992        1993        1994        1995  \\\n",
      "count  185.000000  186.000000  189.000000  189.000000  189.000000  190.000000   \n",
      "mean     4.404504    4.290648    4.174834    4.085472    4.045067    4.136628   \n",
      "std      5.577460    5.577411    5.362451    5.385712    5.464233    5.510400   \n",
      "min      0.001183    0.001158    0.001130    0.001100    0.001071    0.001043   \n",
      "25%      0.434249    0.452759    0.471380    0.448535    0.406659    0.423225   \n",
      "50%      1.914543    1.941825    2.012212    1.867589    1.830972    1.832534   \n",
      "75%      6.910335    6.497471    6.482311    6.430944    6.337175    6.360589   \n",
      "max     30.195189   31.778496   29.632441   33.122025   36.466263   37.102174   \n",
      "\n",
      "             1996        1997        1998        1999  ...        2011  \\\n",
      "count  190.000000  190.000000  189.000000  189.000000  ...  191.000000   \n",
      "mean     4.211147    4.250716    4.282809    4.265652  ...    4.392175   \n",
      "std      5.656146    5.857395    5.846361    5.917358  ...    5.284152   \n",
      "min      0.000000    0.000991    0.035567    0.036699  ...    0.040186   \n",
      "25%      0.461902    0.475029    0.496752    0.509021  ...    0.630991   \n",
      "50%      1.875031    2.034502    2.156377    2.132253  ...    2.492730   \n",
      "75%      6.581746    6.478028    6.369907    6.233211  ...    6.269888   \n",
      "max     40.074177   47.429575   48.045021   50.833850  ...   33.494413   \n",
      "\n",
      "             2012        2013        2014        2015        2016        2017  \\\n",
      "count  191.000000  191.000000  191.000000  191.000000  191.000000  191.000000   \n",
      "mean     4.440399    4.326678    4.226973    4.184129    4.195432    4.199802   \n",
      "std      5.277192    5.203024    5.079421    4.985068    4.938351    4.869959   \n",
      "min      0.040017    0.026979    0.029121    0.039370    0.030715    0.035013   \n",
      "25%      0.633505    0.675626    0.669278    0.667341    0.728493    0.791952   \n",
      "50%      2.562000    2.542694    2.620389    2.579300    2.597787    2.609976   \n",
      "75%      6.336345    6.069882    5.814026    5.776685    5.802129    5.834060   \n",
      "max     34.188222   32.598940   33.205895   33.043510   32.745888   32.127990   \n",
      "\n",
      "             2018        2019      2019.1  \n",
      "count  191.000000  191.000000  191.000000  \n",
      "mean     4.164970    4.115138    4.115138  \n",
      "std      4.748086    4.714595    4.714595  \n",
      "min      0.037113    0.036986    0.036986  \n",
      "25%      0.795606    0.801764    0.801764  \n",
      "50%      2.584477    2.717624    2.717624  \n",
      "75%      5.770189    5.585162    5.585162  \n",
      "max     31.067533   32.474469   32.474469  \n",
      "\n",
      "[8 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read and preprocess the data\n",
    "df = pd.read_csv('data_mine_code/data_sets/CO2 Emissions Around the World/CO2_emission.csv')\n",
    "years = [str(1990+i) for i in range(26)]\n",
    "\n",
    "# The attributes of the Data Frame\n",
    "print(df.columns)\n",
    "# Columns, Rows => Dimensions of the Data Frame\n",
    "print(df.shape)\n",
    "# Details regarding the DataFrame\n",
    "print(df.describe())\n",
    "\n",
    "# Melt dataframe so that the year attribute columns are joined\n",
    "carbon_emissions = df.melt(id_vars=[\"Country Name\", \"country_code\", \"Region\", \"Indicator Name\"], var_name=\"year\", value_name = \"co2_metric\")\n",
    "\n",
    "fig = pex.choropleth(carbon_emissions, locations='country_code', color='co2_metric', scope='world', animation_frame='year')\n",
    "\n",
    "file_exists = exists(\"choropleth_map.html\")\n",
    "if not file_exists:\n",
    "    fig.write_html('choropleth_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='choropleth_map.html' target='_blank'>choropleth_map.html</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\62823\\Desktop\\data_mining_project_Fall_2023\\choropleth_map.html"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('choropleth_map.html')\n",
    "\n",
    "# In order to view the choropleth_map.html file, you can download the Live preview extention that VSCODE offers. Download that, click on the link, \n",
    "# and it should open a new tab. With this tab open, on the top-right, should show a button with a magnifying glass that will display a preview of the .html\n",
    "# file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should display a choropleth map that composes of a interactive interface that allows you to manipulate the scroll on the bottom that will allow you to see the the co2_metrics per country. If you press the 'play' button on the bottom-left, this will automatically go through the years until the latest recorded data from that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> __Modeling data using D.M. algorithms/techniques/Py Libraries__ </center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Predictions of CO2 metrics:\n",
    "For this, we can use an integrated Python library called ARIMA. ARIMA stands for: <br>\n",
    "##### __Autoregressive (AR) :__ #####\n",
    "- Autoregressive (AR) refers to the model taking into account the relationship between a current observation and its previous observations (lags) in the time series.\n",
    "- It uses past values of the variable being predicted to predict future values. For example, if you're predicting CO2 levels for next year, you might consider the CO2 levels for the past few years. <br>\n",
    "##### __Integrated (I) part:__ #####\n",
    "- Integrated (I) indicates the number of differences needed to make the time series stationary. Stationarity means the mean, variance, and covariance are constant over time. If your data isn’t stationary (trends or seasonality present), you might need to difference it to make it stationary before modeling.\n",
    "- In simpler terms, ARIMA combines information from past observations of a time series to predict future values. It looks at patterns, trends, and seasonal effects within the data to forecast what might come next.\n",
    "##### __Moving Average (MA) part:__ #####\n",
    "- Moving Average (MA) refers to the model using the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "- It considers past forecast errors to improve the accuracy of future predictions. It smooths out random fluctuations in the series.\n",
    "\n",
    "\n",
    "For CO2 emissions per capita, an ARIMA model would analyze how CO2 levels changed over previous years, consider any patterns or trends, and use that information to predict what the CO2 emissions might be in the future.\n",
    "\n",
    "\n",
    "For more information about ARIMA Modeling: https://www.ibm.com/docs/en/db2/9.7?topic=series-time-algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> __Integrating the ARIMA model on our cleaned data__ </center> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT THE ARIMA model onto our data\n",
    "# OUTPUT THE NEW VALUES AS A PLOT OR PRINT\n",
    "# DISPLAY it on the choropleth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>__Conclusion(s)__ </center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Predictions: ####\n",
    "[Insert what you thought of the new data]\n",
    "\n",
    "\n",
    "#### Challenges that we had to Overcome: ####\n",
    "[Insert the challenges Tyler and I had to Overcome]\n",
    "- Time \n",
    "> - Meeting\n",
    "> - Classes\n",
    "> - Personal Stuff\n",
    "\n",
    "- Project Scope\n",
    "> - Underestimated\n",
    "\n",
    "- Other things\n",
    "> - Other projects\n",
    "> - Midterms\n",
    "> - Personal?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
