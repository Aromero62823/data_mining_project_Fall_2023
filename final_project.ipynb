{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>__Final Project: Global Warming__</center> #\n",
    "## <center>__Data Mining (CSCI4502)__</center> ##\n",
    "### <center>__Ravie Starzl__</center> ###\n",
    "### <center>__Tyler Scripps & Angel Romero__</center> ###\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Motivation</center> #####\n",
    "The motivation for us to mine data revolving around the problem that is Global Warming is straight-forward, Global Warming is a big issue just because it is affecting us in a wide variety of ways. This problem needs to be a priority when it comes to global crises. Global warming is a serious topic and must be addressed in a serious manner. Extracting and manipulating data to represent the problems we will experience going forward is our goal in this project, as well as display statistical information revolving around contributing factors of global warming.\n",
    "\n",
    "We wanted to take on the topic of Global Warming because of the overall attention that it gets but without having to look too hard on a topic that barely grazes the minds of others. We also had questions of our own on how the earth is affected by certain factors that are/aren't addressed, this composes of volcanic activity, automobile usage, and human-engineered environments that pollute the air with co2. We wanted to also know who is the bigger producer and why they hold that place. \n",
    "\n",
    "These many questions drove our attention to certain factors that can give us answers on what makes accumulates co2_emmissions and why. This is why we chose our topic.\n",
    "\n",
    "##### <center>Why is Global Warming causing us issues?</center> #####\n",
    "The increase of temperatures has caused many issues, ice-caps melting has led to sea level rise, disruption of ecosystems that would disrupt species mortality rate that reside within, health issues, food and water scarcity, etc. Due to the amount of problems that global warming is already causing, we can represent this data visually or concatenate this to a dataset that would represent it in a much more complex manner.\n",
    "\n",
    "As many people already know, if co2 emmissions arise and Global Warming gets worse, this can drastically change everything around us and it can only go down from here if we don't do anything now.\n",
    "\n",
    "Observing changes in temperature will help us determine who will be most affected by climate change going forward. By combining that data with emissions data we can see how emissions may have localized effects on regional patterns and hopefully provide further predictive ability for events like heat waves or even cold snaps that can have major health impacts on local communities.\n",
    "\n",
    "##### <center>Why should we care?</center> #####\n",
    "Well, we all live on the same planet. All jokes aside, collecting this type of data can be used to determine who is the most responsible when it comes to CO2 emissions output. The amount of specified data can help us speculate what we can predict in the near future, such as future trends, or we can ‘splice’ up CO2 contributions into CO2 factors. Collecting this type of data can help Researchers conduct mitigation methods that would mitigate the amount of CO2 for leading causes of CO2, even if us humans are the main cause. \n",
    "\n",
    "The process will conduct an analysis of the data collected from the datasets from Kaggle.com. We will also conduct research on other factors that contribute to the upscale of global warming; gas-powered vehicles, volcanic eruptions, catastrophic events, environmental changes, CO2 emissions output from each country (if we delve deeper, maybe states?), etc. From this data, we can create a stack of factors that go into a much more comprehensive detailed data set that represents these factors accordingly on a time frame that is represented incrementally.\n",
    "\n",
    "#### <center>Literature Survey</center> ####\n",
    "The daily temperature dataset from kaggle has a significant body of work done by other contributors while the emissions dataset is a little more sparse regarding kaggle. This would include data sets in .csv/JSON/SQL/Jupyter Notebook format, discussions revolving around said dataset/topic, code that others have shared among the public that shows their own manipulation via the dataset given, and competitions that would pay actual money to the person who can manipulate and represent the data the best among other competitors.\n",
    "\n",
    "It’s pretty straightforward that we are going to use the datasets provided from the website that others have recorded but the code is also very interesting and something we might reference. The code shows how others manipulate the data set to get desired output or represent the data in a more intricate manner. This can range from having the x-axis as the time frame in increments and the y-axis as something they choose. It can be represented as different plots and graphs. There were also really cool ideas that involved different use of graphs(such as a radar chart), different correlations that the data represented in coherence with seasons, spectacles from the data they extracted from the dataset (such as future trend lines), etc. Ranking was prevalent in the code of others, where the most contributive country/region would output the most values, especially relating to CO2 emissions.\n",
    "\n",
    "Discussions also serve an important purpose, they provide insight on misunderstandings of the datasets. Sometimes people extend on the information presented and try to help others understand the data in a much more intricate manner.\n",
    "\n",
    "From the Discussions, we were able to see how other people were able to visualize, clean, and manipulate the datasets from the csv_files provided. Everyone had their own take on it and were able to come up with consensuses by "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Initializing all the necessary libraries...</center> ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as pex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import plotly.offline as pyo\n",
    "from os.path import exists\n",
    "\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>__Data Cleaning__</center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>NOTE:</b> All datasets that we are referencing are all cited within the folder named, 'datasets'. Where we display all of the datasets used, where they came from, the authors, and a link to reference the page where we collected the dataset from kaggle from.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Trend Cleaning:\n",
    "We collected the data that we needs in order to display future trends. One of the datasets called, 'CO2_emission.csv' in which we utliized the melt() method from the pandas library. The reason for this is quite straight forward, there seems to be columns that have a year heading from 1990 - 2016. So, we would 'melt' these values together in order to fit into 1 single column, that being the co2_metrics per state/country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Read and preprocess the data\n",
    "df = pd.read_csv('data_mine_code/data_sets/CO2 Emissions Around the World/CO2_emission.csv')\n",
    "years = [str(1990+i) for i in range(26)]\n",
    "\n",
    "# Melt dataframe so that the year attribute columns are joined\n",
    "carbon_emissions = df.melt(id_vars=[\"Country Name\", \"country_code\", \"Region\", \"Indicator Name\"], var_name=\"year\", value_name = \"co2_metric\")\n",
    "\n",
    "fig = pex.choropleth(carbon_emissions, locations='country_code', color='co2_metric', scope='world', animation_frame='year')\n",
    "\n",
    "file_exists = exists(\"preprocessed_choropleth_map.html\")\n",
    "if not file_exists:\n",
    "    fig.write_html('preprocessed_choropleth_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='preprocessed_choropleth_map.html' target='_blank'>preprocessed_choropleth_map.html</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\62823\\Desktop\\data_mining_project_Fall_2023\\preprocessed_choropleth_map.html"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('preprocessed_choropleth_map.html')\n",
    "\n",
    "# In order to view the choropleth_map.html file, you can download the Live preview extention that VSCODE offers. Download that, click on the link, \n",
    "# and it should open a new tab. With this tab open, on the top-right, should show a button with a magnifying glass that will display a preview of the .html\n",
    "# file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should display a choropleth map that composes of a interactive interface that allows you to manipulate the scroll on the bottom that will allow you to see the the co2_metrics per country. If you press the 'play' button on the bottom-left, this will automatically go through the years until the latest recorded data from that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the choropleth map is displaying these values up to 2016. There seems to be something odd about it though. There are random 'gray' areas that seem to display nothing. After reviewing the dataset after melting it together with 2 additional values that weld together the years and their output,as the co2_metrics column/attribute column.\n",
    "\n",
    "We need to replace these values with something that can be consistent without disregarding the data, so that it can be re-used in a later reference when we are trying to display it using our later described algorithm.\n",
    "\n",
    "What value would replace these NaN values? Let us replace these values with the mean of each year column.\n",
    "\n",
    "Why Mean?\n",
    "Well, it isn't necessarily he 'best' way to integrate these values in NaN's place. But it does maintain the sample size and is statistically reasonable. \n",
    "\n",
    "I would like to maintain the sample size and make sure there are values that reference that time period's temperature mean to reference later. \n",
    "\n",
    "Drawbacks?\n",
    "\n",
    "It would ignore relationships between variable that reside on the same continent. An example of this would be if the mean of that year's co2_metrics produce a value much closer to the east, it wouldn't really fit with the values of the west.\n",
    "\n",
    "Either way, maintaining our sample size is our priority to reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='postprocessed_choropleth_map.html' target='_blank'>postprocessed_choropleth_map.html</a><br>"
      ],
      "text/plain": [
       "c:\\Users\\62823\\Desktop\\data_mining_project_Fall_2023\\postprocessed_choropleth_map.html"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retreiving the NAN values from the Dataframe\n",
    "means = [df[year].mean(skipna=True) for year in years]\n",
    "\n",
    "# Replacing the NAN values within the Dataframe\n",
    "for i in range(len(years)):\n",
    "    df[years[i]] = df[years[i]].replace(np.nan, means[i])\n",
    "\n",
    "# Initializing the new 'cleaned' data\n",
    "carbon_emissions_proc = df.melt(id_vars=[\"Country Name\", \"country_code\", \"Region\", \"Indicator Name\"], var_name=\"year\", value_name = \"co2_metric\")\n",
    "\n",
    "# carbon_emissions2 = carbon_emissions2[carbon_emissions2['year'] != '2019.1']\n",
    "carbon_emissions_proc = carbon_emissions_proc[carbon_emissions_proc['year'] != '2019.1']\n",
    "\n",
    "# Same on the choropleth graph\n",
    "fig = pex.choropleth(carbon_emissions_proc, locations='country_code', color='co2_metric', scope='world', animation_frame='year')\n",
    "\n",
    "file_exists = exists(\"postprocessed_choropleth_map.html\")\n",
    "if not file_exists:\n",
    "    fig.write_html('postprocessed_choropleth_map.html')\n",
    "\n",
    "FileLink('postprocessed_choropleth_map.html')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Data is now 'cleaned', let us move on to the next part of our algorithm, forecasting future temperatures usign the ARIMA model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> __Modeling data using D.M. algorithms/techniques/Py Libraries__ </center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future Predictions of CO2 metrics:\n",
    "For this, we can use an integrated Python library called ARIMA. ARIMA stands for: <br>\n",
    "##### __Autoregressive (AR) :__ #####\n",
    "- Autoregressive (AR) refers to the model taking into account the relationship between a current observation and its previous observations (lags) in the time series.\n",
    "- It uses past values of the variable being predicted to predict future values. For example, if you're predicting CO2 levels for next year, you might consider the CO2 levels for the past few years. <br>\n",
    "##### __Integrated (I) part:__ #####\n",
    "- Integrated (I) indicates the number of differences needed to make the time series stationary. Stationarity means the mean, variance, and covariance are constant over time. If your data isn’t stationary (trends or seasonality present), you might need to differentiate it to make it stationary before modeling.\n",
    "- In simpler terms, ARIMA combines information from past observations of a time series to predict future values. It looks at patterns, trends, and seasonal effects within the data to forecast what might come next.\n",
    "##### __Moving Average (MA) part:__ #####\n",
    "- Moving Average (MA) refers to the model using the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
    "- It considers past forecast errors to improve the accuracy of future predictions. It smooths out random fluctuations in the series.\n",
    "\n",
    "\n",
    "For CO2 emissions per capita, an ARIMA model would analyze how CO2 levels changed over previous years, consider any patterns or trends, and use that information to predict what the CO2 emissions might be in the future.\n",
    "\n",
    "\n",
    "For more information about ARIMA Modeling: https://www.ibm.com/docs/en/db2/9.7?topic=series-time-algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> __Integrating the ARIMA model on our cleaned data__ </center> ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6450    4.319769\n",
      "6451    4.319769\n",
      "6452    4.319769\n",
      "6453    4.319769\n",
      "6454    4.319769\n",
      "Name: predicted_mean, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning:\n",
      "\n",
      "No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning:\n",
      "\n",
      "No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert 'year' column to datetime with the appropriate format\n",
    "carbon_emissions_proc['year'] = pd.to_datetime(carbon_emissions_proc['year'], errors='coerce', format='%Y')\n",
    "\n",
    "# Set 'year' as the index\n",
    "carbon_emissions_proc.set_index('year', inplace=True)\n",
    "\n",
    "# Train ARIMA model\n",
    "p, d, q = 0, 0, 0 # Example\n",
    "model = ARIMA(carbon_emissions_proc['co2_metric'], order=(d, p, q))\n",
    "arima_results = model.fit()\n",
    "\n",
    "# Make predictions for the next few years (e.g., until 2023)\n",
    "forecast = arima_results.forecast(steps=5)\n",
    "\n",
    "# Print or use forecasted values\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be looking at this code without understanding all of the functionalities. First, we are cleaning our data, we are detecting Nan values and replacing it with the mean values we get from the main dataframe where we extraded our data from. \n",
    "\n",
    "Next, we are the filtering the years, from the choropleth graph that you may have viewed, you may notice that it extends all the way to 2019.1. This is confusing and the values produced are the same as 2019. We will be filtering it out.\n",
    "\n",
    "Finally, it's the forecasting algorithm part where it will use the ARIMA model to forecast future values based on what we have viewed. This can vary depending on the p, d, and q values. Now, what are the purpose of these values? \n",
    "\n",
    "<center>p = log observations captured by the model, from 1 -> length of the recorded logs. -> AR\n",
    "\n",
    "d = degree of differencing. Makes the series stationary. -> I\n",
    "\n",
    "q = captures the dependency between an observation and a residual error from a moving average model applied to lagged observations. -> MA</center>\n",
    "\n",
    "How can we determine the best values for p, d, and q? Let us iterate through the whole dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries in the dataset: 5590\n",
      "Total entries per year:  215\n"
     ]
    }
   ],
   "source": [
    "len_per_year = []\n",
    "for year in years:\n",
    "    len_per_year.append(len(carbon_emissions[carbon_emissions['year'] == year]))\n",
    "    \n",
    "total = sum(len_per_year)\n",
    "print(f\"Total entries in the dataset: {total}\")\n",
    "print(f\"Total entries per year:  {len_per_year[0]}\") # Same for every index value\n",
    "\n",
    "p_values = 50 # How many values to refer back to, let's refer to the last year recorded\n",
    "d_values = 0 # Predictability\n",
    "q_values = 0 # Learning from our errors\n",
    "\n",
    "# Let's say we have no errors, just to see the output of our data.\n",
    "new_order = (p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning:\n",
      "\n",
      "A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: ValueWarning:\n",
      "\n",
      "No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "\n",
      "c:\\Users\\62823\\IdeaProjects\\Homework3_AI\\pyapps\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:836: FutureWarning:\n",
      "\n",
      "No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6450    1.699439\n",
      "6451    6.454567\n",
      "6452    5.202128\n",
      "6453    5.671278\n",
      "6454    4.904037\n",
      "          ...   \n",
      "6660    4.280032\n",
      "6661    4.269358\n",
      "6662    4.279404\n",
      "6663    4.285329\n",
      "6664    4.312158\n",
      "Name: predicted_mean, Length: 215, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "model = ARIMA(carbon_emissions_proc['co2_metric'], order=new_order)\n",
    "arima_results = model.fit()\n",
    "\n",
    "# Make predictions for the next few years (e.g., until 2023) which in this example, is the next year\n",
    "forecast = arima_results.forecast(steps=215)\n",
    "\n",
    "# Print or use forecasted values\n",
    "print(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>__Conclusion(s)__ </center> ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Future Predictions: ####\n",
    "All temperatures will revolve around a general temperature but never be the same since every country isn't located near the equator, where the termperatures are the highest. The highest revolving temperature can increase while the surrounding countries increase their own temperatures.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyapps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
